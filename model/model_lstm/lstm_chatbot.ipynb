{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OFHPq57wntC6"
      },
      "outputs": [],
      "source": [
        "# import csv\n",
        "# file = open(\"WHO_FAQ.csv\")\n",
        "# csvreader = csv.reader(file)\n",
        "# header = next(csvreader)\n",
        "# print(header)\n",
        "# rows = []\n",
        "# for row in csvreader:\n",
        "#     rows.append(row)\n",
        "# print(rows)\n",
        "# file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in rows:\n",
        "#   print('{\"pats\":[\"'+str(i[0])+'\"],\\n\"replies\":[\"'+str(i[1]) +'\"]},\\n')"
      ],
      "metadata": {
        "id": "jel_3yUgoAP3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nui72q5Zl_2O",
        "outputId": "70b21e5a-884b-499b-9149-3d4f4ce5c4da"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import numpy as np \n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "20GHU9eupQ_A"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "LYLPyeSBGPkm"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz1H-4wzmByc",
        "outputId": "4ac2516e-ac1e-47c4-cf8a-f1aa18412427"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc_list=['.','/','\"','-','?',',',\"'\"]\n",
        "covid_vars=[\"COVID\", \"Corona\", \"Coronavirus\", \"COVID-19\"]\n",
        "exclude_list=[\"not\",\"nor\",\"don't\",\"what\",\"how\",\"are\",\"you\"]\n",
        "stop_words = [i for i in stopwords.words('english') if (i not in exclude_list)]\n",
        "\n",
        "def preprocess(example_sent):\n",
        "  example_sent=example_sent.replace(\"covid-19\",\"covid\").replace(\"coronavirus\",\"covid\").replace(\"corona\",\"covid\").replace(\"covid19\",\"covid\").replace(\"virus\",\"covid\")\n",
        "  word_tokens = word_tokenize(example_sent)\n",
        "  filtered_sentence = [stemmer.stem(w) for w in word_tokens if not w.lower() in stop_words]\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "  for i in punc_list:\n",
        "    if i in filtered_sentence:\n",
        "      filtered_sentence.remove(i)\n",
        "  return ' '.join(filtered_sentence)"
      ],
      "metadata": {
        "id": "bewkjdKJmjeQ"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('intents.json') as file:\n",
        "    data = json.load(file)\n",
        "    \n",
        "train_x = []\n",
        "train_y = []\n",
        "bag = []\n",
        "replies = []\n",
        "\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pat in intent['patterns']:\n",
        "        s=preprocess(pat)\n",
        "        train_x.append(s.lower())\n",
        "        print(s.lower())\n",
        "        train_y.append(intent['tag'].lower())\n",
        "    replies.append([i.lower() for i in intent['responses']])\n",
        "    \n",
        "    if intent['tag'] not in bag:\n",
        "        bag.append(intent['tag'].lower())\n",
        "        \n",
        "num_classes = len(bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVdEXUcSqdXd",
        "outputId": "4a60b3b7-779b-4db9-f565-513b609e1ed3"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n",
            "hey\n",
            "is anyone\n",
            "hello\n",
            "hay\n",
            "sup\n",
            "how are you\n",
            "how are you\n",
            "are you fine\n",
            "how you\n",
            "how life\n",
            "how you\n",
            "bye\n",
            "see you later\n",
            "goodbye\n",
            "thanks\n",
            "thank you\n",
            "that 's helpful\n",
            "thanks help\n",
            "okay\n",
            "ok\n",
            "who are you\n",
            "what are you\n",
            "who\n",
            "what name\n",
            "what i call you\n",
            "whats name\n",
            "your name\n",
            "could you help\n",
            "give hand please\n",
            "can you help\n",
            "what you\n",
            "i need support\n",
            "i need help\n",
            "support please\n",
            "what covid\n",
            "what covid\n",
            "what covid\n",
            "what covid\n",
            "what pandemic\n",
            "what pandemic\n",
            "what covid\n",
            "what covid\n",
            "what covid\n",
            "what covid\n",
            "what covid\n",
            "what are symptoms covid\n",
            "symptoms covid\n",
            "what happens covid\n",
            "what happens you get covid\n",
            "what are symptoms covid\n",
            "get fever covid\n",
            "what happens i get covid\n",
            "what get covid\n",
            "what are symptoms covid\n",
            "what are symptoms covid\n",
            "fever symptom covid\n",
            "cough symptom covid\n",
            "do old people get covid faster\n",
            "how covid rise\n",
            "how covid increase\n",
            "how people catch covid\n",
            "how peole get covid\n",
            "how covid spread\n",
            "how covid spread\n",
            "covid spread\n",
            "how people catch covid\n",
            "how covid spread\n",
            "can covid spread air\n",
            "can covid move air\n",
            "covid spread air\n",
            "can covid spread coughing\n",
            "can covid causes covid transmitted air\n",
            "how covid transmit\n",
            "how covid spread people\n",
            "is covid air borne\n",
            "is covid transmitted air\n",
            "can covid caught person symptoms\n",
            "what reason covid spreads\n",
            "why covid spread\n",
            "can covid caught person symptoms\n",
            "what are symptoms covid\n",
            "everyone experience strong symptoms covid\n",
            "can i catch covid feces someone disease\n",
            "can covid spread washroom\n",
            "covid spread feces\n",
            "how wash hands properly\n",
            "what remove germs hands\n",
            "what i protect prevent spread disease\n",
            "how stop covid\n",
            "how prevent covid\n",
            "how not get infected\n",
            "how stop infection\n",
            "how improve immunity\n",
            "how stay safe\n",
            "how not catch covid\n",
            "how not catch covid\n",
            "how stop infection\n",
            "how clean hands\n",
            "what i protect prevent spread disease\n",
            "how protect covid\n",
            "how protext covid\n",
            "how stop disease spreading\n",
            "how stop covid\n",
            "how remove covid\n",
            "what i protect prevent spread disease\n",
            "touch nose\n",
            "touch eyes without getting infected\n",
            "tough mouth without getting infected\n",
            "infection spread mouth\n",
            "what i protect prevent spread disease\n",
            "what are precautions covid\n",
            "how not get covid\n",
            "what i protect prevent spread disease\n",
            "stay home\n",
            "staying home good\n",
            "lockdown necessary\n",
            "what lockdown\n",
            "lockdown\n",
            "quarantine\n",
            "what quarantine\n",
            "quarantining help stop covid\n",
            "what i protect prevent spread disease\n",
            "what i protect prevent spread disease\n",
            "how protect\n",
            "how protect others\n",
            "how protect others\n",
            "how prevent covid\n",
            "how protect covid\n",
            "how prevent covid\n",
            "what i protect andprotection measures persons are recently visited ( past 14 days ) areas covid spreading prevent spread disease\n",
            "what first contact\n",
            "what near positive person\n",
            "what not well\n",
            "what sick\n",
            "measures take sick\n",
            "how protect ourself sick\n",
            "how protect others sick\n",
            "protection measures persons are recently visited ( past 14 days ) areas covid spreading\n",
            "what are protection measures\n",
            "what covid prone area\n",
            "what travelled recently\n",
            "what cough\n",
            "what breathing difficulty\n",
            "what difficult breathe\n",
            "breathing difficulty covid symptom\n",
            "fever symptom\n",
            "how likely i catch covid\n",
            "how check covid\n",
            "how check positive\n",
            "positive\n",
            "how know are positive\n",
            "how see covid\n",
            "how check someone covid\n",
            "how know covid\n",
            "how likely i catch covid\n",
            "covid\n",
            "covid positive\n",
            "how know positive\n",
            "how test home\n",
            "how likely i catch covid\n",
            "how prevent outbreak\n",
            "how spread outbreak\n",
            "what outbreak\n",
            "should i worry covid\n",
            "scared\n",
            "scared covid\n",
            "covid dangerous\n",
            "covid mild\n",
            "should i worry covid\n",
            "covid risky\n",
            "concerned\n",
            "covid lead death\n",
            "who risk developing severe illness\n",
            "are likely get covid\n",
            "get covid\n",
            "are likely infected\n",
            "get covid first\n",
            "are antibiotics effective preventing treating covid\n",
            "are antibiotics effective\n",
            "take antibiotics\n",
            "take medicines\n",
            "i eat tablets\n",
            "medicines\n",
            "take medicine prevent covid\n",
            "take medicine cure covid\n",
            "are antibiotics effective preventing treating covid\n",
            "supposed take medicines\n",
            "taking medicines\n",
            "are medicines therapies prevent cure covid\n",
            "how cure covid\n",
            "what cures covid\n",
            "how get well fast\n",
            "how get rid symptoms\n",
            "are medicines therapies prevent cure covid\n",
            "medicines help\n",
            "medicines cure covid\n",
            "therapy cure covid\n",
            "therapy help get covid\n",
            "is vaccine drug treatment covid\n",
            "what vaccine take\n",
            "cure covid\n",
            "vaccine work\n",
            "vaccine good\n",
            "is vaccine drug treatment covid\n",
            "covid cured\n",
            "drugs help covid\n",
            "what treatment covid\n",
            "treatment covid\n",
            "is vaccine drug treatment covid\n",
            "what are vaccines\n",
            "what vaccines covid\n",
            "is vaccine drug treatment covid\n",
            "is covid sars\n",
            "covid sars\n",
            "what difference covid sars\n",
            "covid sars are\n",
            "is covid sars\n",
            "should i wear mask protect\n",
            "wearing mask helpful\n",
            "wearing mask useful\n",
            "wearing mask work\n",
            "wearing mask help\n",
            "wear mask\n",
            "wearing mask helpful\n",
            "mask prevent covid\n",
            "mask protect us covid\n",
            "masks protext us covid\n",
            "should i wear mask protect\n",
            "wear mask im positive\n",
            "wear mask im negative\n",
            "should i wear mask protect\n",
            "shortage masks\n",
            "how put use , take dispose mask\n",
            "how use mask\n",
            "how wear mask\n",
            "how use mask properly\n",
            "proper usage mask\n",
            "how wear mask properly\n",
            "how dispose mask\n",
            "throw mask\n",
            "what masks\n",
            "how put use , take dispose mask\n",
            "take mask\n",
            "how long incubation period covid\n",
            "how long need quarantine\n",
            "how much quarantine important\n",
            "how long stay lockdown\n",
            "positive how long stay home\n",
            "how long incubation period covid\n",
            "how long quarantine period covid\n",
            "can humans infected covid animal source\n",
            "get infected animals\n",
            "animals spread covid\n",
            "animals get covid\n",
            "animals give us covid\n",
            "animals give us covid\n",
            "can humans infected covid animal source\n",
            "get covid animal source\n",
            "can humans infected covid animal source\n",
            "can humans infected covid animal source\n",
            "can humans infected covid animal source\n",
            "covid come animal source\n",
            "covid caught animals\n",
            "can i catch covid pet\n",
            "get covid pet\n",
            "pet covid get\n",
            "pet covid get infected\n",
            "get infected pet covid\n",
            "pet give covid\n",
            "can i catch covid pet\n",
            "how long covid survive surfaces\n",
            "how long covid stay surface\n",
            "covid stick surfaces\n",
            "covid surface\n",
            "covid furniture\n",
            "covid stick furniture\n",
            "how long covid survive surfaces\n",
            "covid stick surface\n",
            "covid stay surface long\n",
            "how long covid survive surfaces\n",
            "is safe receive package area covid reported\n",
            "take parcel covid positive person\n",
            "get covid delivery boy\n",
            "unsafe order online covid\n",
            "unsafe order online\n",
            "get covid parcel\n",
            "is anything i not\n",
            "prevention measures covid\n",
            "smoking good\n",
            "is anything i not\n",
            "is anything i not\n",
            "what not\n",
            "what stay away\n",
            "is source causing covid known\n",
            "covid come\n",
            "how covid born\n",
            "covid\n",
            "how covid came existence\n",
            "how first human sars-cov-2 infections occur\n",
            "what first covid case\n",
            "how covid start\n",
            "how infection start\n",
            "how covid start\n",
            "the first known infections sars-cov-2 discovered wuhan china the original source viral transmission humans remains unclear , whether covid became pathogenic spillover event .\n",
            "how first human sars-cov-2 infections occur\n",
            "how first human sars-cov-2 infections occur\n",
            "is covid airborne\n",
            "covid spread air\n",
            "covid move air\n",
            "covid spread air\n",
            "covid spread air\n",
            "covid spread air\n",
            "covid move across air\n",
            "air transmit covid\n",
            "are pregnant women higher risk covid\n",
            "are pregnant women risk\n",
            "pregnant women catch covid faster\n",
            "pregnant women catch covid faster\n",
            "are pregnant women less immune\n",
            "are pregnant women higher risk covid\n",
            "pregnant women catch covid fast\n",
            "pregnant women catch covid\n",
            "i 'm pregnant how i protect covid\n",
            "i 'm pregnant how i protect covid\n",
            "how prevent covid pregnancy\n",
            "should pregnant women tested covid\n",
            "safe test covid pregnant\n",
            "safe test pregnant\n",
            "can covid passed woman unborn baby\n",
            "covid happen unborn baby\n",
            "covid infect unborn baby\n",
            "what care available pregnancy childbirth\n",
            "childbirth risky covid\n",
            "do pregnant women suspected confirmed covid need give birth ceasarean section\n",
            "can women covid breastfeed\n",
            "can i touch hold newborn baby i covid\n",
            "covid spread newborn baby\n",
            "baby covid\n",
            "baby get covid touch\n",
            "how are covid influenza covides similar\n",
            "covid influenza\n",
            "how are covid influenza covides similar\n",
            "how are covid influenza covides different\n",
            "are covid influenza different\n",
            "how are covid influenza covides different\n",
            "how are covid influenza covides different\n",
            "what medical interventions are available covid influenza covides\n",
            "what medical cures covid\n",
            "how cure covid medically\n",
            "are smokers tobacco users higher risk covid infection\n",
            "smoking okay\n",
            "smoking risky\n",
            "are smokers less immune\n",
            "smoking increase immunity\n",
            "are smokers tobacco users higher risk covid infection\n",
            "are tobacco users less immune\n",
            "tobacco good health\n",
            "how large meeting event need order mass gathering\n",
            "how big mass gathering\n",
            "how many people are mass gathering\n",
            "what mass gathering\n",
            "how large meeting event need order mass gathering\n",
            "does who recommend international mass gatherings cancelled covid\n",
            "not meet many people\n",
            "mass gathering good\n",
            "okay\n",
            "thank you\n",
            "bye\n",
            "bubye\n",
            "do i need get vaccine\n",
            "are vaccine doses compulsory\n",
            "how get vaccine\n",
            "vaccine\n",
            "how get vaccination\n",
            "vaccination necessary\n",
            "vaccination helpful\n",
            "what are covid variants\n",
            "what are variants\n",
            "different covid variants\n",
            "covid variants\n",
            "how long recover\n",
            "how fast recover covid\n",
            "recover covid\n",
            "how much time takes recover\n",
            "time recover\n",
            "how increase immunity\n",
            "what eat covid\n",
            "what eat stop covid\n",
            "suggest diets covid\n",
            "how better immunity\n",
            "what eat prevent covid\n",
            "what food good covid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(train_y)\n",
        "train_y = lbl_encoder.transform(train_y)"
      ],
      "metadata": {
        "id": "wvF_ozmZrpMA"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "oov_token = \"<OOV>\"\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_x)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "AtfLmmw2sMcQ"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model.add(layers.LSTM(128))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSXE6oOfsADl",
        "outputId": "c8082864-e228-4349-a2ad-4338194af398"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_13 (Embedding)    (None, 50, 16)            160000    \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 128)               74240     \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 40)                5160      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 239,400\n",
            "Trainable params: 239,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnxSusLfyAHH",
        "outputId": "b704762c-914b-42b6-df56-95f48fedc6d7"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12 12 12 12 12 12  4  4  4  4  4  4  7  7  7 36 36 36 36 36 36  0  0  0\n",
            " 18 18 18 18 13 13 13 13 13 13 13  9  9  9  9  9  9  9  9  9  9  9 35 35\n",
            " 35 35 35 35 35 35 35 20 20 20 20 33 33 33 33 33 33 33 33 33 33 33 33 33\n",
            " 33 33 33 33 33 33 33 33 35 35 35 35 35 35 24 24 24 24 24 24 24 24 24 24\n",
            " 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
            " 24 24 24 29 29 29 29 29 29 29  8  8  8  8  8  8  8  8 27 27 27 27 27 27\n",
            " 27 27 27 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n",
            " 28 28 28 28 28 28 28 28 28 28  3  3  3  3  3  3  3  3  3  3  3 17 17 17\n",
            " 17 17 17 17 17 17 17 37 37 37 37 37 17 17 17 17 17 17 17 17 17 30 30 30\n",
            " 30 30 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
            " 15 15 15 15 25 25 25 25 25 25 25  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
            " 22 22 22 22 22 22 22 34 34 34 34 34 34 34 34 34 34 21 21 21 21 21 21 19\n",
            " 19 19 19 19 19 19 32 32 32 32 32 10 10 10 10 10 10 10 10  1  1  1  1  1\n",
            "  1  1  1 23 23 23 23 23 23 23 23 23 23 23 23 23 23  5  5  5 23 23 23  6\n",
            "  5  5  5  5 14 14 14 14 14 14 14 16 16 16 31 31 31 31 31 31 31 31 11 11\n",
            " 11 11 11 11 11 11  7  7  7  7 37 37 37 37 37 37 37 38 38 38 38 26 26 26\n",
            " 26 26  9  9  9  9  9  9  9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "history = model.fit(padded_sequences, np.array(train_y), epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TwQmhtysJau",
        "outputId": "c4acbee0-b123-4606-caef-8bf35b409b70"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "13/13 [==============================] - 3s 89ms/step - loss: 3.6607 - accuracy: 0.0687\n",
            "Epoch 2/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 3.4962 - accuracy: 0.0941\n",
            "Epoch 3/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 3.4612 - accuracy: 0.0941\n",
            "Epoch 4/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 3.4498 - accuracy: 0.0941\n",
            "Epoch 5/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 3.4324 - accuracy: 0.0941\n",
            "Epoch 6/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 3.4336 - accuracy: 0.0636\n",
            "Epoch 7/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 3.4233 - accuracy: 0.0941\n",
            "Epoch 8/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 3.4090 - accuracy: 0.0941\n",
            "Epoch 9/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 3.3972 - accuracy: 0.0941\n",
            "Epoch 10/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 3.3727 - accuracy: 0.1323\n",
            "Epoch 11/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 3.3260 - accuracy: 0.1221\n",
            "Epoch 12/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 3.2444 - accuracy: 0.1705\n",
            "Epoch 13/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 3.1262 - accuracy: 0.2087\n",
            "Epoch 14/150\n",
            "13/13 [==============================] - 1s 92ms/step - loss: 2.9209 - accuracy: 0.2443\n",
            "Epoch 15/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 2.7439 - accuracy: 0.3257\n",
            "Epoch 16/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 2.5449 - accuracy: 0.3435\n",
            "Epoch 17/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 2.3426 - accuracy: 0.4275\n",
            "Epoch 18/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 2.1437 - accuracy: 0.4427\n",
            "Epoch 19/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 2.0028 - accuracy: 0.4962\n",
            "Epoch 20/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 1.8149 - accuracy: 0.5216\n",
            "Epoch 21/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 1.6603 - accuracy: 0.5700\n",
            "Epoch 22/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 1.5151 - accuracy: 0.6285\n",
            "Epoch 23/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 1.4356 - accuracy: 0.6539\n",
            "Epoch 24/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 1.2882 - accuracy: 0.6921\n",
            "Epoch 25/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 1.1676 - accuracy: 0.7099\n",
            "Epoch 26/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 1.0825 - accuracy: 0.7455\n",
            "Epoch 27/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.9861 - accuracy: 0.7812\n",
            "Epoch 28/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.9150 - accuracy: 0.7964\n",
            "Epoch 29/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.8542 - accuracy: 0.8244\n",
            "Epoch 30/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.7745 - accuracy: 0.8321\n",
            "Epoch 31/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.7263 - accuracy: 0.8677\n",
            "Epoch 32/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.6693 - accuracy: 0.8804\n",
            "Epoch 33/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.6297 - accuracy: 0.8753\n",
            "Epoch 34/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.5794 - accuracy: 0.9059\n",
            "Epoch 35/150\n",
            "13/13 [==============================] - 1s 83ms/step - loss: 0.5475 - accuracy: 0.9109\n",
            "Epoch 36/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.5094 - accuracy: 0.9109\n",
            "Epoch 37/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.4727 - accuracy: 0.9364\n",
            "Epoch 38/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.4437 - accuracy: 0.9262\n",
            "Epoch 39/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.4059 - accuracy: 0.9262\n",
            "Epoch 40/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.3851 - accuracy: 0.9338\n",
            "Epoch 41/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.3771 - accuracy: 0.9415\n",
            "Epoch 42/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.3616 - accuracy: 0.9415\n",
            "Epoch 43/150\n",
            "13/13 [==============================] - 1s 92ms/step - loss: 0.3374 - accuracy: 0.9466\n",
            "Epoch 44/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.3208 - accuracy: 0.9542\n",
            "Epoch 45/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.2926 - accuracy: 0.9517\n",
            "Epoch 46/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.2824 - accuracy: 0.9644\n",
            "Epoch 47/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.2609 - accuracy: 0.9618\n",
            "Epoch 48/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.2471 - accuracy: 0.9517\n",
            "Epoch 49/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.2384 - accuracy: 0.9644\n",
            "Epoch 50/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.2346 - accuracy: 0.9542\n",
            "Epoch 51/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.2233 - accuracy: 0.9593\n",
            "Epoch 52/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.2097 - accuracy: 0.9593\n",
            "Epoch 53/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.2037 - accuracy: 0.9669\n",
            "Epoch 54/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.2028 - accuracy: 0.9618\n",
            "Epoch 55/150\n",
            "13/13 [==============================] - 1s 92ms/step - loss: 0.1921 - accuracy: 0.9618\n",
            "Epoch 56/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.1804 - accuracy: 0.9593\n",
            "Epoch 57/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1783 - accuracy: 0.9618\n",
            "Epoch 58/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1767 - accuracy: 0.9695\n",
            "Epoch 59/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.1678 - accuracy: 0.9669\n",
            "Epoch 60/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1637 - accuracy: 0.9644\n",
            "Epoch 61/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.1607 - accuracy: 0.9593\n",
            "Epoch 62/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.1484 - accuracy: 0.9695\n",
            "Epoch 63/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.1470 - accuracy: 0.9669\n",
            "Epoch 64/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.1400 - accuracy: 0.9669\n",
            "Epoch 65/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.1358 - accuracy: 0.9644\n",
            "Epoch 66/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1312 - accuracy: 0.9720\n",
            "Epoch 67/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.1301 - accuracy: 0.9695\n",
            "Epoch 68/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1275 - accuracy: 0.9746\n",
            "Epoch 69/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.1233 - accuracy: 0.9695\n",
            "Epoch 70/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.1235 - accuracy: 0.9695\n",
            "Epoch 71/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.1167 - accuracy: 0.9695\n",
            "Epoch 72/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.1171 - accuracy: 0.9720\n",
            "Epoch 73/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.1160 - accuracy: 0.9695\n",
            "Epoch 74/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.1154 - accuracy: 0.9695\n",
            "Epoch 75/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1126 - accuracy: 0.9720\n",
            "Epoch 76/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.1105 - accuracy: 0.9720\n",
            "Epoch 77/150\n",
            "13/13 [==============================] - 1s 84ms/step - loss: 0.1120 - accuracy: 0.9746\n",
            "Epoch 78/150\n",
            "13/13 [==============================] - 1s 93ms/step - loss: 0.1171 - accuracy: 0.9644\n",
            "Epoch 79/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.1092 - accuracy: 0.9669\n",
            "Epoch 80/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.1124 - accuracy: 0.9720\n",
            "Epoch 81/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.1002 - accuracy: 0.9746\n",
            "Epoch 82/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0993 - accuracy: 0.9618\n",
            "Epoch 83/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0959 - accuracy: 0.9695\n",
            "Epoch 84/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0973 - accuracy: 0.9746\n",
            "Epoch 85/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0970 - accuracy: 0.9720\n",
            "Epoch 86/150\n",
            "13/13 [==============================] - 1s 92ms/step - loss: 0.0974 - accuracy: 0.9746\n",
            "Epoch 87/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0992 - accuracy: 0.9669\n",
            "Epoch 88/150\n",
            "13/13 [==============================] - 1s 83ms/step - loss: 0.0969 - accuracy: 0.9720\n",
            "Epoch 89/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0885 - accuracy: 0.9695\n",
            "Epoch 90/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0961 - accuracy: 0.9695\n",
            "Epoch 91/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0878 - accuracy: 0.9695\n",
            "Epoch 92/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0873 - accuracy: 0.9746\n",
            "Epoch 93/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0841 - accuracy: 0.9695\n",
            "Epoch 94/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0836 - accuracy: 0.9720\n",
            "Epoch 95/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0885 - accuracy: 0.9746\n",
            "Epoch 96/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0851 - accuracy: 0.9720\n",
            "Epoch 97/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0868 - accuracy: 0.9695\n",
            "Epoch 98/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0837 - accuracy: 0.9669\n",
            "Epoch 99/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0841 - accuracy: 0.9669\n",
            "Epoch 100/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0818 - accuracy: 0.9720\n",
            "Epoch 101/150\n",
            "13/13 [==============================] - 1s 93ms/step - loss: 0.0806 - accuracy: 0.9720\n",
            "Epoch 102/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0811 - accuracy: 0.9695\n",
            "Epoch 103/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0789 - accuracy: 0.9746\n",
            "Epoch 104/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0804 - accuracy: 0.9720\n",
            "Epoch 105/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.0782 - accuracy: 0.9720\n",
            "Epoch 106/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0768 - accuracy: 0.9695\n",
            "Epoch 107/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0778 - accuracy: 0.9644\n",
            "Epoch 108/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0798 - accuracy: 0.9720\n",
            "Epoch 109/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0782 - accuracy: 0.9720\n",
            "Epoch 110/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0788 - accuracy: 0.9720\n",
            "Epoch 111/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0760 - accuracy: 0.9669\n",
            "Epoch 112/150\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 0.0748 - accuracy: 0.9720\n",
            "Epoch 113/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0750 - accuracy: 0.9669\n",
            "Epoch 114/150\n",
            "13/13 [==============================] - 1s 95ms/step - loss: 0.0744 - accuracy: 0.9669\n",
            "Epoch 115/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0759 - accuracy: 0.9720\n",
            "Epoch 116/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0781 - accuracy: 0.9695\n",
            "Epoch 117/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0717 - accuracy: 0.9720\n",
            "Epoch 118/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0774 - accuracy: 0.9695\n",
            "Epoch 119/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0735 - accuracy: 0.9720\n",
            "Epoch 120/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0728 - accuracy: 0.9720\n",
            "Epoch 121/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0720 - accuracy: 0.9669\n",
            "Epoch 122/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0720 - accuracy: 0.9669\n",
            "Epoch 123/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0711 - accuracy: 0.9720\n",
            "Epoch 124/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0896 - accuracy: 0.9593\n",
            "Epoch 125/150\n",
            "13/13 [==============================] - 1s 93ms/step - loss: 0.0782 - accuracy: 0.9746\n",
            "Epoch 126/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0734 - accuracy: 0.9669\n",
            "Epoch 127/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0697 - accuracy: 0.9695\n",
            "Epoch 128/150\n",
            "13/13 [==============================] - 1s 91ms/step - loss: 0.0694 - accuracy: 0.9695\n",
            "Epoch 129/150\n",
            "13/13 [==============================] - 1s 92ms/step - loss: 0.0709 - accuracy: 0.9720\n",
            "Epoch 130/150\n",
            "13/13 [==============================] - 1s 94ms/step - loss: 0.0688 - accuracy: 0.9746\n",
            "Epoch 131/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0723 - accuracy: 0.9644\n",
            "Epoch 132/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0697 - accuracy: 0.9669\n",
            "Epoch 133/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0674 - accuracy: 0.9720\n",
            "Epoch 134/150\n",
            "13/13 [==============================] - 1s 94ms/step - loss: 0.0668 - accuracy: 0.9746\n",
            "Epoch 135/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0713 - accuracy: 0.9695\n",
            "Epoch 136/150\n",
            "13/13 [==============================] - 1s 92ms/step - loss: 0.0667 - accuracy: 0.9669\n",
            "Epoch 137/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0707 - accuracy: 0.9720\n",
            "Epoch 138/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0649 - accuracy: 0.9644\n",
            "Epoch 139/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0662 - accuracy: 0.9720\n",
            "Epoch 140/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0673 - accuracy: 0.9695\n",
            "Epoch 141/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0662 - accuracy: 0.9669\n",
            "Epoch 142/150\n",
            "13/13 [==============================] - 1s 90ms/step - loss: 0.0660 - accuracy: 0.9669\n",
            "Epoch 143/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0724 - accuracy: 0.9746\n",
            "Epoch 144/150\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.0664 - accuracy: 0.9746\n",
            "Epoch 145/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0662 - accuracy: 0.9720\n",
            "Epoch 146/150\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.0709 - accuracy: 0.9746\n",
            "Epoch 147/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0725 - accuracy: 0.9669\n",
            "Epoch 148/150\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.0689 - accuracy: 0.9695\n",
            "Epoch 149/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0673 - accuracy: 0.9720\n",
            "Epoch 150/150\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 0.0662 - accuracy: 0.9720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KMeFBYzQtguQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"lstm_covid_chatbot_model\")\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# to save the fitted tokenizer\n",
        "with open('tokenizer_lstm.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "# to save the fitted label encoder\n",
        "with open('label_encoder_lstm.pickle', 'wb') as ecn_file:\n",
        "    pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amKPnP1xtd5O",
        "outputId": "5994e9b9-1289-4e75-cc43-5ed719f8d442"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_6_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: lstm_covid_chatbot_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: lstm_covid_chatbot_model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3d89820590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r lstm_chatmodel.zip lstm_covid_chatbot_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCCALRxBFROm",
        "outputId": "32035b86-d0d2-44f2-bf35-f3677868746f"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: lstm_covid_chatbot_model/ (stored 0%)\n",
            "  adding: lstm_covid_chatbot_model/variables/ (stored 0%)\n",
            "  adding: lstm_covid_chatbot_model/variables/variables.index (deflated 61%)\n",
            "  adding: lstm_covid_chatbot_model/variables/variables.data-00000-of-00001 (deflated 47%)\n",
            "  adding: lstm_covid_chatbot_model/assets/ (stored 0%)\n",
            "  adding: lstm_covid_chatbot_model/saved_model.pb (deflated 90%)\n",
            "  adding: lstm_covid_chatbot_model/keras_metadata.pb (deflated 86%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "_AoX8g2rt1bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bTwAwhlv75R",
        "outputId": "6f6990e4-b9c0-4ad6-9c37-aadc4e779dbe"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import colorama \n",
        "colorama.init()\n",
        "from colorama import Fore, Style, Back\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "with open(\"intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "\n",
        "def chat():\n",
        "    # load trained model\n",
        "    # model = keras.models.load_model('covid_chatbot_model9')\n",
        "\n",
        "    # # load tokenizer object\n",
        "    # with open('tokenizer9.pickle', 'rb') as handle:\n",
        "    #     tokenizer = pickle.load(handle)\n",
        "\n",
        "    # # load label encoder object\n",
        "    # with open('label_encoder9.pickle', 'rb') as enc:\n",
        "    #     lbl_encoder = pickle.load(enc)\n",
        "\n",
        "    # parameters\n",
        "    max_len = 50\n",
        "    \n",
        "    while True:\n",
        "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
        "        inp = input()\n",
        "        s=preprocess(inp)\n",
        "        inp=s.lower()\n",
        "        print(tokenizer.texts_to_sequences([inp]))\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
        "                                             truncating='post', maxlen=max_len))\n",
        "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
        "\n",
        "        for i in data['intents']:\n",
        "            if i['tag'] == tag[0].lower():\n",
        "                print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL , np.random.choice(i['responses']))\n",
        "\n",
        "        # print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL,random.choice(replies))\n",
        "\n",
        "print(Fore.YELLOW + \"Start messaging with the bot (type quit to stop)!\" + Style.RESET_ALL)\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxWWS6G6u0aL",
        "outputId": "b6cd2d08-0a5f-4b36-aadc-d004ed702b85"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start messaging with the bot (type quit to stop)!\n",
            "User: hey\n",
            "[[183]]\n",
            "ChatBot: Hi there\n",
            "User: How are you doing?\n",
            "[[3, 5, 14]]\n",
            "ChatBot: Im fine!\n",
            "User: What is corona?\n",
            "[[4, 2]]\n",
            "ChatBot: Coronaviruses are a large family of viruses which may cause illness in animals or humans. \n",
            "ChatBot: covid is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019.\n",
            "ChatBot: Avoid smoking and eat healthy\n",
            "User: How to protect myself?\n",
            "[[3, 10]]\n",
            "ChatBot: Keep up to date on the latest covid hotspots (cities or local areas where covid is spreading widely).\n",
            "User: should I wear mask if i am positive\n",
            "[[6, 32, 9, 27]]\n",
            "ChatBot: Only wear a mask if you are ill with covid symptoms (especially coughing) or looking after someone who may have covid. Disposable face mask can only be used once.\n",
            "ChatBot: If you are not ill or looking after someone who is ill then you are wasting a mask. \n",
            "ChatBot: There is a world-wide shortage of masks, so WHO urges people to use masks wisely.\n",
            "ChatBot: Remember, a mask should only be used by health workers, care takers, and individuals with respiratory symptoms, such as fever and cough.\n",
            "ChatBot: Before touching the mask, clean hands with an alcohol-based hand rub or soap and water\n",
            "User: thank you\n",
            "[[124, 14]]\n",
            "ChatBot: Any time!\n",
            "User: bye\n",
            "[[121]]\n",
            "ChatBot: Bye! Come back again\n",
            "ChatBot: Great! See you later!\n",
            "User: quit\n",
            "[[1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MsbSFK24_KEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fGLGmU-IC4bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr=0\n",
        "tot=0\n",
        "for x in data['intents']:\n",
        "  for inp in x['patterns']:\n",
        "    tot+=1\n",
        "    s=preprocess(inp)\n",
        "    inp=s.lower()\n",
        "\n",
        "    result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([inp]),\n",
        "                                          truncating='post', maxlen=max_len))\n",
        "    tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
        "    if tag[0].lower()==x['tag'].lower():\n",
        "      corr+=1"
      ],
      "metadata": {
        "id": "KUug8QCEvgWX"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Correct: {}\\nTotal: {}\".format(corr,tot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2QmX8FMMTIP",
        "outputId": "587691c7-5bb7-400b-a9af-169afad15d69"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 383\n",
            "Total: 393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: {}\".format(corr*100/tot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZJ4QAtlMvmz",
        "outputId": "f83f426d-f242-4a36-c358-1ce8ae588618"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.45547073791349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "epTtQC8rQjd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}